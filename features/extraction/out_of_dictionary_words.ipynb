{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b999bdb4f965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringMatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyprind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "import pickle\n",
    "import sys\n",
    "from Levenshtein.StringMatcher import StringMatcher\n",
    "import pyprind\n",
    "\n",
    "sys.path.insert(1, '../../dataset')\n",
    "import dataset_map_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Method for loading dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_map_entry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b6c2dc53f3f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b6c2dc53f3f8>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../dataset/output/map_final.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_map_entry'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_map_entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_map_entry' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    input_dataset = open('../../dataset/output/map_final.pkl', 'rb')\n",
    "    sys.modules['dataset_map_entry'] = dataset_map_entry\n",
    "    dataset = pickle.load(input_dataset)\n",
    "    input_dataset.close()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Creating dictionary (and everything else that is necessary) for storing counts of 'out of dictionary words' with respect to the total number of words. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary with english words\n",
    "d = enchant.Dict(\"en_US\")\n",
    "# dictionary with spanish words (because they may occurr in english tweets)\n",
    "d_sp = enchant.Dict(\"es\")\n",
    "# Tool for calculating Levenshtein word distance\n",
    "sm = StringMatcher()\n",
    "# List with out of dictionary words\n",
    "out_of_dict_words = set()\n",
    "# List with words in dictionary\n",
    "in_dict_words = set()\n",
    "# Levensthein word distance ratio threshold\n",
    "WORD_DIST_RATIO = 0.6\n",
    "#load brands\n",
    "brands = set(line.lower().strip() for line in open('brands.txt'))\n",
    "# dictionary with (user, (no_of_out_of_dict_words, total_no_of_words)) pair\n",
    "user_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Storing pairs (no_of_out_dict_words, total_no_of_words) in dictionary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:37\n"
     ]
    }
   ],
   "source": [
    "pBar = pyprind.ProgBar(len(dataset))\n",
    "for user in dataset:\n",
    "    for tweet in dataset[user].tweets:\n",
    "        for word in tweet:\n",
    "            if word.replace('\\'','').replace('-','').isalpha() and not d.check(word) \\\n",
    "             and not d_sp.check(word) and (word not in brands) and ('URL' not in word) and ('NUMBER' not in word):\n",
    "                sm.set_seq1(seq1=word)\n",
    "                founded = False\n",
    "                for suggestion in d.suggest(word):\n",
    "                    sm.set_seq2(seq2=suggestion)\n",
    "                    if sm.ratio > WORD_DIST_RATIO:\n",
    "                        out_of_dict_words.add(word)\n",
    "                        founded = True\n",
    "                        break\n",
    "                if not founded:\n",
    "                    in_dict_words.add(word)\n",
    "            else:\n",
    "                in_dict_words.add(word)     \n",
    "    user_map[user] = (len(out_of_dict_words), len(out_of_dict_words) + len(in_dict_words))\n",
    "    out_of_dict_words.clear()\n",
    "    in_dict_words.clear()\n",
    "    pBar.update()            \n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. Storing dictionary in a file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = open('../pkls/out_of_dict_map.pkl', 'wb')\n",
    "pickle.dump(user_map, output_dir)\n",
    "output_dir.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
